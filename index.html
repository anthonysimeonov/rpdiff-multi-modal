<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Shelving, Stacking, Hanging: Relational Pose Diffusion for Multi-modal Rearrangement">
    <meta name="author" content="Anthony Simeonov,
                                Ankit Goyal,
                                Lucas Manuelli,
                                Lin Yen-Chen,
                                Alina Sarmiento,
                                Alberto Rodriguez,
                                Pulkit Agrawal,
                                Dieter Fox">

    <title>Shelving, Stacking, Hanging: Relational Pose Diffusion for Multi-modal Rearrangement</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>Shelving, Stacking, Hanging: Relational Pose</br> Diffusion for Multi-modal Rearrangement</h2>
    <h3>arXiv 2023</h3>
    <hr>
    <p class="authors">
        <a href="https://anthonysimeonov.github.io/"> Anthony Simeonov</a>,
        <a href="https://imankgoyal.github.io/"> Ankit Goyal*</a>,
        <a href="http://lucasmanuelli.com/"> Lucas Manuelli*</a>,
        <a href="https://yenchenlin.me/"> Lin Yen-Chen</a>,</br>
        <a href="https://www.linkedin.com/in/alina-sarmiento/"> Alina Sarmiento</a>,
        <a href="https://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU"> Alberto Rodriguez</a>,
        <a href="http://people.csail.mit.edu/pulkitag/"> Pulkit Agrawal**</a>,
        <a href="https://homes.cs.washington.edu/~fox/"> Dieter Fox**</a></br>
        Massachusetts Institute of Technology, NVIDIA Research</br>
    </p>
    <p>
        *Equal contribution, **Equal advising </br>Work done in part during NVIDIA Research internship
    </p>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2307.04751">Paper</a>
        <a class="btn btn-primary" href="https://github.com/anthonysimeonov/rpdiff">Code</a>
    </div>
</div>

<div class="container">
    <div class="section">
        <div class="vcontainer">
            <iframe class='video' src="https://www.youtube.com/embed/e6_4wtuUfJw" frameborder="0"
                    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen></iframe>
        </div>
        <hr>
        <p>
            We propose a system for rearranging objects in a scene to achieve a desired object-scene placing relationship, 
            such as a book inserted in an open slot of a bookshelf. The pipeline generalizes to novel geometries, poses, 
            and layouts of both scenes and objects, and is trained from demonstrations to operate directly on 3D point clouds. 
            
            Our system overcomes challenges associated with the existence of many geometrically-similar rearrangement solutions 
            for a given scene. By leveraging an iterative pose de-noising training procedure, we can fit multi-modal demonstration 
            data and produce multi-modal outputs while remaining precise and accurate. We also show the advantages of conditioning 
            on relevant local geometric features while ignoring irrelevant global structure that harms both generalization and 
            precision. 
            
            We demonstrate our approach on three distinct rearrangement tasks that require handling multi-modality and 
            generalization over object shape and pose in both simulation and the real world.
        </p>
    </div>

    <div class="section">
        <h2>Test-time Inference via Iterative Pose De-noising</h2>
        <hr>
        <p>
            Iterative pose de-noising for unseen simulated objects at test-time. Starting from diverse initial 
            guess configurations of the objects relative to the scene, the inference process converges to a
            diverse set of final output rearrangement solutions.
        </p>
        <h3>Book/Bookshelf</h3>
        <div class="row justify-content-center">
            <div class="col-md-4">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/bookshelf_miscviz_k5_2_c.mp4" type="video/mp4">
                </video>
            </div> 
            <div class="col-md-4">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/bookshelf_miscviz_k5_3_c.mp4" type="video/mp4">
                </video>
            </div> 
            <div class="col-md-4">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/bookshelf_miscviz_k5_4_c.mp4" type="video/mp4">
                </video>
            </div> 
        </div> 

        <h3>Mug/Rack-multi</h3>
        <div class="row justify-content-center">
            <div class="col-md-4">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/mugrack_miscviz_k5_2_c.mp4" type="video/mp4">
                </video>
            </div> 
            <div class="col-md-4">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/mugrack_miscviz_k5_3_c.mp4" type="video/mp4">
                </video>
            </div> 
            <div class="col-md-4">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/mugrack_miscviz_k5_1_c.mp4" type="video/mp4">
                </video>
            </div> 
        </div> 

        <h3>Can/Cabinet</h3>
        <div class="row justify-content-center">
            <div class="col-md-4">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/cancabinet_miscviz_k5_2_c.mp4" type="video/mp4">
                </video>
            </div> 
            <div class="col-md-4">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/cancabinet_miscviz_k5_3_c.mp4" type="video/mp4">
                </video>
            </div> 
            <div class="col-md-4">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/cancabinet_miscviz_k5_1_c.mp4" type="video/mp4">
                </video>
            </div> 
        </div> 
    </div>

    <div class="section">
        <h2>Real-world Multi-modal Rearrangement via Pick-and-Place</h2>
        <hr>
        <p>
            Rearrangement in the real world using the Franka Panda arm. Each task features scene 
            objects that offer multiple placement locations. RPDiff is used to produce a set of 
            candidate placements and one of the predicted solutions is executed. Multiple 
            executions in sequence show the ability to find multiple diverse solutions. Our 
            neural network is trained in simulation and directly deployed in the real world 
            (we do observe some performance gap due to sim2real distribution shift). 
        </p>
        <!-- <h3>Book/Bookshelf</h3> -->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <h3>Book/Bookshelf</h3>
                <video width="60%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/book_bookshelf_seq1_8x_rdc_lout.mp4" type="video/mp4">
                </video>
            </div> 
        </div>

        <!-- <h3>Mug/Rack-multi</h3> -->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <h3>Mug/Rack-multi</h3>
                <video width="60%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/mug_rack_multi_seq1_8x_lout.mp4" type="video/mp4">
                </video>
            </div> 
        </div>

        <!-- <h3>Can/Cabinet</h3> -->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <h3>Can/Cabinet</h3>
                <video width="60%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/can_cabinet_seq1_10x_lout.mp4" type="video/mp4">
                </video>
            </div> 
        </div>
    </div>

    <div class="section">
        <h2>Training by perturbing object-scene point clouds and predicting corrective SE(3) transforms</h2>
        <hr>
        <p>
            We train on examples of point clouds showing properly configured object-scene pairs, obtained
            from procedurally generated rearrangement demonstrations on simulated objects. Training 
            targets are generated by creating object point clouds with sequences of perturbation 
            transforms applied. The network is trained to take in the noised object-scene point cloud and 
            predict an SE(3) transform to apply to the object that takes a step back toward the original 
            configuration. 
            
            We crop the scene point cloud to improve generalization and precision by ignoring 
            faraway details that are irrelevant for prediction and re-use features that describe local 
            scene geometry across instances and spatial regions. 
        </p>
        <div class="row justify-content-center">
            <div class="col-md-6">
                <img src='img/rpdiff-train-img.png' style="width:100%">
            </div> 
            <div class="col-md-6">
                <img src="img/rpdiff-net-img.png" style="width:100%">
            </div> 
        </div> 
    </div>

    <div class="section">
        <h2>Our Related Projects</h2>
        <hr>
        <p>
            Check out our related projects on the topic of object rearrangement and local scene conditioning <br>
        </p>
        <div class='row vspace-top'>
            <div class="col-sm-3">
                <img src='img/related/rndf_bowl_bottle_notext.gif' class='img-fluid'>
            </div>

            <div class="col">
                <div class='paper-title'>
                    <a href="http://anthonysimeonov.github.io/r-ndf/">SE(3)-Equivariant Relational Rearrangement with Neural Descriptor Fields</a>
                </div>
                <div>
                    We use neural descriptor fields (NDFs) to represent pairs of objects, label task-relevant local coordinate frames 
                    and detect correspondencing coordinate frames on unseen instances in arbitrary initial poses. NDFs enable this to work
                    for novel object instances from just a handful of demonstrations. This enables relational rearrangement with pairs of
                    unseen objects by aligning the detected frames to each other.
                </div>
            </div>
        </div>

        <div class='row vspace-top'>
            <div class="col-sm-3">
                <img src='img/related/ifor_real_seq_1ed_c.gif' class='img-fluid'>
            </div>

            <div class="col">
                <div class='paper-title'>
                    <a href="https://imankgoyal.github.io/ifor.html">IFOR: Iterative Flow Minimization for Robotic Object Rearrangement</a>

                </div>
                <div>
                    An end-to-end method for object rearrangement of unknown objects given an RGBD image of the original and final scenes. 
                    IFOR first learns an optical flow model to estimate relative object transformations from synthetic data. This flow is then 
                    used in an iterative minimization algorithm to achieve accurate positioning of previously unseen objects in cluttered scenes, 
                    and in the real world (while training only on synthetic data). 
                </div>
            </div>
        </div>

        <div class='row vspace-top'>
            <div class="col-sm-3">
                <img src='img/related/lndf_2023.gif' class='img-fluid'>
            </div>

            <div class="col">
                <div class='paper-title'>
                    <a href="https://elchun.github.io/lndf/">Local Neural Descriptor Fields: Locally Conditioned Object Representations for Manipulation</a>
                </div>
                <div>
                    Local Neural Descriptor Fields (L-NDF), utilizes neural descriptors defined on the local geometry of the object to 
                    transfer manipulation demonstrations to novel objects at test time. By encoding geometry that is local to a spatial 
                    region rather than associating with an entire objects, L-NDF performs better with clutter and occlusions, and enables 
                    interacting with familiar object parts that are attached to unfamiliar objects.  
                </div>
            </div>
        </div>

    <div class="section">
        <h2>External Related Projects</h2>
        <hr>
        <p>
            Check out other projects related to diffusion models, iterative prediction, and rearrangement<br>
        </p>
        <div class='row vspace-top'>
            <div class="col-sm-3">
                <img src='img/external/structdiff.png' class='img-fluid'>
            </div>

            <div class="col">
                <div class='paper-title'>
                    <a href="https://structdiffusion.github.io/">StructDiffusion: Language-Guided Creation of Physically-Valid Structures using Unseen Objects</a>
                </div>
                <div>
                    Combines a diffusion model and an object-centric transformer to construct structures given 
                    partial-view point clouds and high-level language goals, such as "set the table" and "make a line".
                    Using use one multi-task model, this allows building physically-valid structures without 
                    step-by-step instructions. 
                </div>
            </div>
        </div>

        <div class='row vspace-top'>
            <div class="col-sm-3">
                <img src='img/external/legonet.png' class='img-fluid'>
            </div>

            <div class="col">
                <div class='paper-title'>
                    <a href="https://ivl.cs.brown.edu/#/projects/lego-net">LEGO-Net: Learning Regular Rearrangements of Objects in Rooms</a>
                </div>
                <div>
                    A data-driven transformer-based iterative method for learning reeglar rearrangement of objects 
                    in messy rooms. Partly inspired by diffusion models, LEGO-Net starts with an initial messy 
                    state and iteratively denoises the position and orientation of objects to a regular state while 
                    reducing distance traveled.
                </div>
            </div>
        </div>

        <div class='row vspace-top'>
            <div class="col-sm-3">
                <img src='img/external/se3diff2.png' class='img-fluid'>
            </div>

            <div class="col">
                <div class='paper-title'>
                    <a href="https://arxiv.org/abs/2209.03855">SE(3)-DiffusionFields: Learning smooth cost functions for joint grasp and motion optimization through diffusion</a>
                </div>
                <div>
                    A method for learning data-driven SE(3) cost functions as diffusion models, which allows integration 
                    with other costs into a single differentiable objective function. Specifically focused on learning SE(3) 
                    diffusion models for 6-DoF grasping, this framework enables joint grasp and motion optimization 
                    without needing to decouple grasp selection from trajectory generation.
                </div>
            </div>
        </div>

        <div class='row vspace-top'>
            <div class="col-sm-3">
                <img src='img/external/indi2.png' class='img-fluid'>
            </div>

            <div class="col">
                <div class='paper-title'>
                    <a href="https://arxiv.org/abs/2303.11435">Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration</a>
                </div>
                <div>
                    A formulation for image restoration that avoids the ``regression to the mean'' effect by gradually 
                    improving image quality, similar to generative denoising diffusion models. Rather 
                    than predicting in a single step, which can result in an aggregation of all possible restoration 
                    explanations, InDI instead iteratively improves the image in small steps, resulting in better perceptual quality. 
                </div>
            </div>
        </div>

    <div class="section">
        <h2>Paper</h2>
        <hr>
        <div>
            <div class="list-group">
                <a href="https://arxiv.org/abs/2307.04751"
                   class="list-group-item">
                    <img src="img/paper-thumb.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
                </a>
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
            @article{simeonov2023rpdiff,
                author = {Simeonov, Anthony
                            and Goyal, Ankit
                            and Manuelli, Lucas
                            and Yen-Chen, Lin
                            and Sarmiento, Alina,
                            and Rodriguez, Alberto
                            and Agrawal, Pulkit
                            and Fox, Dieter},
                title = {Shelving, Stacking, Hanging: Relational
                            Pose Diffusion for Multi-modal Rearrangement},
                journal={arXiv preprint arXiv:2307.04751},
                year={2023}
            }
        </div>
    </div>

    <hr>

    <footer>
        <h2>Acknowledgements</h2>
        <p>
            We would like to thank NVIDIA Seattle Robotics Lab members and the MIT Improbable AI Lab for their valuable feedback and support in developing this project. 
            In particular, we would like to acknowledge Idan Shenfeld, Anurag Ajay, and Antonia Bronars for helpful suggestions on improving the clarity of the draft. 
            This work was partly supported by Sony Research Awards and Amazon Research Awards. Anthony Simeonov is supported in part by the NSF Graduate Research Fellowship.
        </p>
        <p>Send feedback and questions to <a href="https://anthonysimeonov.github.io">Anthony Simeonov</a></p>
        <div class="row justify-content-center">
        <p>Website template recycled from <a href="https://www.vincentsitzmann.com/siren/">SIREN</a></p>
        </div>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
